{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "GNN final project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erezimm/ML-ML/blob/master/GNN_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCU4DoTLIIoO",
        "colab_type": "text"
      },
      "source": [
        "# Main notebook, to be run on Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWYU8-BTJ-XN",
        "colab_type": "text"
      },
      "source": [
        "## Import dependencies and download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5E9Y_nnn3m8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install dgl\n",
        "\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import astropy.io.ascii\n",
        "import dgl\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise Exception('Cuda unavailable, turn it on under Runtime>Change runtime type>GPU')\n",
        "device = torch.device('cpu')\n",
        "\n",
        "training_path = '/content/training_set'\n",
        "validation_path = '/content/validation_set'\n",
        "for path in (training_path, validation_path):\n",
        "    if not os.path.isdir(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "!wget -O data.zip https://github.com/erezimm/ML-ML/raw/master/data.zip\n",
        "!unzip -o -q data.zip -d /content/\n",
        "data_path = '/content/data'\n",
        "microlist = glob.glob(data_path+'/microlensedconst_*')\n",
        "varlist = glob.glob(data_path+'/cleanvar_*')\n",
        "for typelist in (microlist, varlist):\n",
        "    for i, f in enumerate(typelist):\n",
        "        dest = training_path if i<len(microlist)/2 else validation_path\n",
        "        shutil.move(f, os.path.join(dest, os.path.basename(f)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff2ASGtVn3nB",
        "colab_type": "text"
      },
      "source": [
        "## Create Datasets\n",
        "Class CustomDataset loads all files and converts them to a list of graphs.\n",
        "Each node in a graph is a data point, i.e. node features are days since start, mag, magerr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSLI4uNYn3nB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        filelist = glob.glob(path+'/*')\n",
        "        \n",
        "        self.graphs = []\n",
        "        self.isMicrolensed = []\n",
        "        for fname in tqdm(filelist):\n",
        "            filebasename = os.path.basename(fname)\n",
        "            if filebasename.startswith('cleanvar'):\n",
        "                microlensed = torch.tensor([0])\n",
        "            elif filebasename.startswith('microlensedconst'):\n",
        "                microlensed = torch.tensor([1])\n",
        "            else:\n",
        "                raise Exception('Filename ' + fname + ' not clean_* or microlensed_*')\n",
        "            \n",
        "            with open(fname) as f:\n",
        "                data = astropy.io.ascii.read(f.read())\n",
        "                times, mags, magerrs = data['col1'], [], data['col3']\n",
        "                for m, err in zip(data['col2'], magerrs):\n",
        "                    mags.append(round(m, len(str(err).split('.')[1])))\n",
        "                zipped = list(zip(times, mags, magerrs))\n",
        "                zipped.sort(key=lambda tup: tup[0])\n",
        "                times, mags, magerrs = zip(*zipped)\n",
        "\n",
        "                n = len(times)\n",
        "                g = dgl.DGLGraph()\n",
        "                g.add_nodes(n)\n",
        "                g.ndata['time'] = torch.tensor(times).float()  # days\n",
        "                g.ndata['mag'] = torch.tensor(mags).float()\n",
        "                g.ndata['magerr'] = torch.tensor(magerrs).float()\n",
        "                g.add_edges([i for i in range(n)], [(i+1)%n for i in range(n)])\n",
        "                g.add_edges([i for i in range(n)], [(i-1)%n for i in range(n)])\n",
        "                \n",
        "                self.graphs.append(g)\n",
        "                self.isMicrolensed.append(microlensed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.graphs[idx], self.isMicrolensed[idx]\n",
        "\n",
        "\n",
        "train_ds = CustomDataset(training_path)\n",
        "validation_ds = CustomDataset(validation_path)\n",
        "\n",
        "nx.draw(dgl.to_networkx(train_ds[20][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnD7mWfjn3nK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate(samples):\n",
        "    # The input `samples` is a list, a batch of whatever comes out of your dataset object\n",
        "    graphs = [x[0] for x in samples]\n",
        "    labels = [x[1] for x in samples]\n",
        "    \n",
        "    batched_graph = dgl.batch(graphs, node_attrs=['time', 'mag', 'magerr'])\n",
        "    targets = torch.cat(labels)\n",
        "    \n",
        "    return batched_graph, targets.unsqueeze(1).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGDTIcKIuGW2",
        "colab_type": "text"
      },
      "source": [
        "## The Newtwork"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs14d7QUn3nL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_ft = 3  # number of node features (time, mag, magerr)\n",
        "\n",
        "class EdgeNetwork(nn.Module):\n",
        "    def __init__(self, node_hidrep, edge_hidrep):\n",
        "        super(EdgeNetwork, self).__init__()\n",
        "        n_in = 2*node_hidrep\n",
        "        \n",
        "        # network:\n",
        "        sizes = [n_in] + [] + [edge_hidrep]\n",
        "\n",
        "        layers = []\n",
        "        for i in range(len(sizes)-1):\n",
        "            layers.extend([nn.Linear(sizes[i], sizes[i+1]), nn.ReLU()])\n",
        "        self.network = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        input = torch.cat((x.dst['node_hidrep'], x.src['node_hidrep']), dim=1)\n",
        "        output = self.network(input)\n",
        "        return {'edge_hidrep': output }\n",
        "\n",
        "    \n",
        "class NodeNetwork(nn.Module):\n",
        "    def __init__(self, node_hidrep, edge_hidrep):\n",
        "        super(NodeNetwork, self).__init__()\n",
        "        n_in = edge_hidrep + node_hidrep\n",
        "\n",
        "        # network:\n",
        "        sizes = [n_in] + [] + [node_hidrep]\n",
        "\n",
        "        layers = []\n",
        "        for i in range(len(sizes)-1):\n",
        "            layers.extend([nn.Linear(sizes[i], sizes[i+1]), nn.ReLU()])\n",
        "        self.network = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mb = torch.mean( x.mailbox['edge_hidrep'] , dim=1 )\n",
        "        input = torch.cat((mb, x.data['node_hidrep']), dim=1)\n",
        "        out = self.network(input)\n",
        "        return {'node_hidrep': out }\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        # networks:\n",
        "        node_hidrep, edge_hidrep = 10, 10\n",
        "        sizes_nodeinit = [n_ft] + [] + [node_hidrep]\n",
        "        self.update_loop_length = 20\n",
        "\n",
        "        layers = []\n",
        "        for i in range(len(sizes_nodeinit)-1):\n",
        "            layers.extend([nn.Linear(sizes_nodeinit[i], sizes_nodeinit[i+1]), nn.ReLU()])\n",
        "        self.node_init = nn.Sequential(*layers)\n",
        "        self.edge_network = EdgeNetwork(node_hidrep, edge_hidrep)\n",
        "        self.node_network = NodeNetwork(node_hidrep, edge_hidrep)\n",
        "        \n",
        "    def forward(self, g):\n",
        "        features = torch.tensor(list(zip(g.ndata['time'], g.ndata['mag'], g.ndata['magerr'])))\n",
        "        g.ndata['node_hidrep'] = self.node_init(features)\n",
        "\n",
        "        for i in range(self.update_loop_length):\n",
        "            g.update_all(self.edge_network, self.node_network)\n",
        "        \n",
        "        # eventually take max of all entries in all nodes (?), maybe you can apply functions with dgl.function ?\n",
        "        output = dgl.max_nodes(g, 'node_hidrep')\n",
        "        output = torch.max(output, 1, keepdim=True)[0]\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddlu-kVsuqGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Classifier()\n",
        "net.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x-ALDRlwBDo",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgrXh0Jdn3nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(net, validation_ds):\n",
        "    test_data_loader = DataLoader(validation_ds, batch_size=30, shuffle=True, collate_fn=collate)\n",
        "\n",
        "    net.cpu()\n",
        "    net.eval()\n",
        "\n",
        "    true_positive = 0\n",
        "    false_positive = 0\n",
        "    true_negative = 0\n",
        "    false_negative = 0\n",
        "    total_positives = 0\n",
        "    total_negatives = 0\n",
        "\n",
        "    for i, (x,y) in enumerate(test_data_loader):\n",
        "            y = y.data.numpy()\n",
        "            prediction = net(x).cpu().data.numpy()\n",
        "            \n",
        "            prediction[prediction >= 0.5] = 1\n",
        "            prediction[prediction <0.5] = 0\n",
        "            \n",
        "            total_positives+=len(np.where( y==1 )[0])\n",
        "            total_negatives+=len(np.where( y==0 )[0])\n",
        "            \n",
        "            true_positive+= len(np.where( (prediction==y) & (y==1) )[0])\n",
        "            # true_negative+= len(np.where( (prediction==y) & (y==0) )[0])\n",
        "            false_positive+= len(np.where( (prediction!=y) & (y==0) )[0])\n",
        "            # false_negative+= len(np.where( (prediction!=y) & (y==1) )[0])\n",
        "                    \n",
        "    print('Probability of detection:', true_positive/total_positives)  # probability of recognizing a microlensing when you see it\n",
        "    print('Probability of false alarm:', false_positive/total_negatives)  # probability for misclassification of a variable star as microlensing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "E0-E3v84n3nU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# options:\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "n_epochs = 50\n",
        "\n",
        "losses = []\n",
        "\n",
        "data_loader = DataLoader(train_ds, batch_size=30, shuffle=True, collate_fn=collate)\n",
        "for epoch in tqdm(range(n_epochs)):\n",
        "    net.train()\n",
        "    this_losses = []\n",
        "    for x,y in data_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = net(x)\n",
        "        # print(output.shape, y.shape)\n",
        "        loss = loss_func(output,y)\n",
        "        this_losses.append(float(loss))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    losses.append(np.mean(this_losses))\n",
        "    print('Epoch:', epoch, 'Loss:', round(losses[-1], 4))  # for some real-time indication (couldn't manage to get this to dynamically plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU3-bAMS-D-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the printed losses:\n",
        "losses = np.array(losses)\n",
        "plt.plot(losses[1:],label='training loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "evaluate(net, validation_ds)  # show P_D, P_FA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VfHxWg7aL98",
        "colab_type": "text"
      },
      "source": [
        "## Save\n",
        "Don't forget to download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cVQVmottKTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timestamp = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "torch.save(net.state_dict(), 'model_' + timestamp + '.pt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}